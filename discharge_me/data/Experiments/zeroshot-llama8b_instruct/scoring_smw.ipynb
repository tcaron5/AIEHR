{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e59709a0-7786-4714-b0a7-244c00f14a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df15d11a-ad3f-4bcb-95c4-0096d3b98ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discharge instructions for hadm 25142813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a64cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10962, 3)\n",
      "(10962, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# # data_dir = Path(\"..\\\\data\\\\scoring\\\\submission_01\")\n",
    "# refs_fullpath = r\"C:\\Users\\s.wendelken\\OneDrive - Northeastern University\\Projects\\CODE\\discharge_me\\data\\scoring\\submission_01\\submission.csv.csv\"\n",
    "# hyps_fullpath = r\"C:\\Users\\s.wendelken\\OneDrive - Northeastern University\\Projects\\CODE\\discharge_me\\data\\scoring\\submission_01\\discharge_target.csv\"\n",
    "refs_fullpath = 'submission.csv'\n",
    "\n",
    "hyps_fullpath = 'discharge_target.csv'\n",
    "\n",
    "# Load the data\n",
    "refs_df = pd.read_csv(refs_fullpath)\n",
    "hyps_df = pd.read_csv(hyps_fullpath)\n",
    "\n",
    "# hyps_df = pd.read_csv(os.path.join(data_dir, \"targets.csv\"))\n",
    "\n",
    "# results = pd.DataFrame(columns=[\"BLEU\", \"ROUGE_1\", \"ROUGE_2\", \"ROUGE_L\", \"METEOR\", \"CIDEr\", \"BERTScore\", \"BLEURT\"])\n",
    "# refs_df = pd.read_csv()\n",
    "# hyps_df = pd.read_csv(\"C:\\Users\\s.wendelken\\OneDrive - Northeastern University\\Projects\\CODE\\discharge_me\\data\\scoring\\submission_01\\discharge_target.csv\")\n",
    "\n",
    "# hyps_df.head()\n",
    "refs_df.head()\n",
    "print(refs_df.shape)\n",
    "print(hyps_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c3b4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [08:01<00:00, 60.18s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.66it/s]\n",
      "c:\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 482.32 seconds, 0.52 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discharge_instructions:\n",
      "BLEU                0.038683\n",
      "ROUGE-1             0.272309\n",
      "ROUGE-2             0.078433\n",
      "ROUGE-L             0.258637\n",
      "F1 Score (BERT)     0.835737\n",
      "Precision (BERT)    0.833775\n",
      "Recall (BERT)       0.838135\n",
      "METEOR              0.230086\n",
      "dtype: float64\n",
      "overall mean: 0.4232243113085784\n",
      "BLEU                0.038683\n",
      "ROUGE-1             0.272309\n",
      "ROUGE-2             0.078433\n",
      "ROUGE-L             0.258637\n",
      "F1 Score (BERT)     0.835737\n",
      "Precision (BERT)    0.833775\n",
      "Recall (BERT)       0.838135\n",
      "METEOR              0.230086\n",
      "dtype: float64\n",
      "overall mean: 0.4232243113085784\n"
     ]
    }
   ],
   "source": [
    "## scoring script \n",
    "\n",
    "import pandas as pd\n",
    "from bert_score import score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu \n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# Initialize evaluator\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_data(refs_fullpath, hyps_fullpath, cols, data_range):\n",
    "    # Load the data\n",
    "    refs_data = pd.read_csv(refs_fullpath)\n",
    "    hyps_data = pd.read_csv(hyps_fullpath)\n",
    "\n",
    "    refs_df = refs_data[cols][data_range].replace(\"\\n\", \" \") ## set range!!!\n",
    "    hyps_df = hyps_data[cols][data_range].replace(\"\\n\", \" \")\n",
    "\n",
    "    return refs_df, hyps_df\n",
    "\n",
    "def calculate_scores(refs_df, hyps_df):\n",
    "    refs = refs_df.tolist()\n",
    "    hyps = hyps_df.tolist()\n",
    "\n",
    "    # BERTScore Calculation\n",
    "    P, R, F1 = score(cands=hyps, refs=refs, lang=\"en\", verbose=True)\n",
    "    \n",
    " \n",
    "    # BLEU and ROUGE scores initialization\n",
    "    bleu_scores = []\n",
    "    rouge_scores = []\n",
    "    rouge = Rouge()\n",
    "\n",
    "\n",
    "    for ref, hyp in zip(refs, hyps):\n",
    "        # BLEU Score Calculation\n",
    "\n",
    "        bleu_score = sentence_bleu([ref.split()], hyp.split(), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        # bleu_score = corpus_bleu([ref.split()], hyp.split(), smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "\n",
    "        # ROUGE Score Calculation\n",
    "        scores = rouge.get_scores(hyp, ref)[0]\n",
    "        rouge_scores.append(scores)\n",
    "\n",
    "        \n",
    "    # METEOR Score Calculation\n",
    "    # Initialize the METEOR scorer\n",
    "    meteor_scorer = evaluate.load('meteor')\n",
    "    meteor_scores = [meteor_scorer.compute(predictions=[hyp], references=[ref])['meteor'] for ref, hyp in zip(refs, hyps)]\n",
    "\n",
    "    # Adding scores to dataframe\n",
    "    results_df = pd.DataFrame({\n",
    "        # 'discharge_instructions': refs_df['discharge_instructions'],\n",
    "\n",
    "\n",
    "        'BLEU': bleu_scores,\n",
    "        'ROUGE-1': [score['rouge-1']['f'] for score in rouge_scores],\n",
    "        'ROUGE-2': [score['rouge-2']['f'] for score in rouge_scores],\n",
    "        'ROUGE-L': [score['rouge-l']['f'] for score in rouge_scores],\n",
    "        'F1 Score (BERT)': F1.numpy(),\n",
    "        'Precision (BERT)': P.numpy(),\n",
    "        'Recall (BERT)': R.numpy(),\n",
    "        'METEOR': meteor_scores\n",
    "    })\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Load the data from discharge instructions\n",
    "cols = 'discharge_instructions'\n",
    "data_range = slice(0, 250)\n",
    "refs_df, hyps_df = load_data(refs_fullpath, hyps_fullpath, cols, data_range)\n",
    "results = calculate_scores(refs_df, hyps_df)\n",
    "# print(results)\n",
    "print('Discharge_instructions:')\n",
    "mean_scores = results.mean()\n",
    "print(mean_scores) \n",
    "print(\"overall mean: \" + str(mean_scores.mean()))\n",
    "# selected_metrics = ['F1 Score (BERT)', 'BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'METEOR']\n",
    "# print(\"overall mean: \" + str(mean_scores[selected_metrics].mean()))\n",
    "scores_dishcarge_instructions = results\n",
    "mean_scores_discharge_instructions = mean_scores\n",
    "# average the scores for only selected metrics\n",
    "overall_scores_discharge_instructions = mean_scores.mean()\n",
    "\n",
    "print(mean_scores_discharge_instructions) \n",
    "mean_scores_bhc.to_csv('mean_scores_discharge_instructions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2297116e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [08:40<00:00, 65.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.49it/s]\n",
      "c:\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 521.39 seconds, 0.48 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU                0.026207\n",
      "ROUGE-1             0.229334\n",
      "ROUGE-2             0.068346\n",
      "ROUGE-L             0.216008\n",
      "F1 Score (BERT)     0.821601\n",
      "Precision (BERT)    0.805791\n",
      "Recall (BERT)       0.838565\n",
      "METEOR              0.241789\n",
      "dtype: float64\n",
      "overall mean: 0.4059550385421707\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the data from brief_hospital_course\n",
    "cols = 'brief_hospital_course'\n",
    "data_range = slice(0, 250)\n",
    "refs_df, hyps_df = load_data(refs_fullpath, hyps_fullpath, cols, data_range)\n",
    "results = calculate_scores(refs_df, hyps_df)\n",
    "mean_scores = results.mean()\n",
    "print(mean_scores) \n",
    "print(\"overall mean: \" + str(mean_scores.mean()))\n",
    "# selected_metrics = ['F1 Score (BERT)', 'BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'METEOR']\n",
    "# print(\"overall mean: \" + str(mean_scores[selected_metrics].mean()))\n",
    "scores_brief_hospital_course = results\n",
    "mean_scores_bhc= mean_scores\n",
    "# average the scores for only selected metrics\n",
    "overall_scores_brief_hospital_course = mean_scores.mean()\n",
    "\n",
    "print(mean_scores_bhc) \n",
    "mean_scores_bhc.to_csv('mean_scores_bhc.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16bca8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU                0.038683\n",
      "ROUGE-1             0.272309\n",
      "ROUGE-2             0.078433\n",
      "ROUGE-L             0.258637\n",
      "F1 Score (BERT)     0.835737\n",
      "Precision (BERT)    0.833775\n",
      "Recall (BERT)       0.838135\n",
      "METEOR              0.230086\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(mean_scores_discharge_instructions) \n",
    "mean_scores_bhc.to_csv('mean_scores_discharge_instructions.csv')\n",
    "print(mean_scores_discharge_instructions) \n",
    "mean_scores_bhc.to_csv('mean_scores_discharge_instructions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f320080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU                0.026207\n",
      "ROUGE-1             0.229334\n",
      "ROUGE-2             0.068346\n",
      "ROUGE-L             0.216008\n",
      "F1 Score (BERT)     0.821601\n",
      "Precision (BERT)    0.805791\n",
      "Recall (BERT)       0.838565\n",
      "METEOR              0.241789\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# print(results)\n",
    "# mean_scores = results.mean()\n",
    "print(mean_scores_bhc) \n",
    "mean_scores_bhc.to_csv('mean_scores_bhc.csv')\n",
    "# print(\"overall mean: \" + str(mean_scores.mean()))\n",
    "# scores_brief_hosptial_course = results\n",
    "# mean_scores_brief_hospital_course = mean_scores\n",
    "# selected_metrics = ['F1 Score (BERT)', 'BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'METEOR']\n",
    "# overall_scores_brief_hospital_course = mean_scores[selected_metrics].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3fcb5a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8430268168449402\n"
     ]
    }
   ],
   "source": [
    "print(mean_scores_discharge_instructions['F1 Score (BERT)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9de02a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall mean discharge_instructions: 0.4053250370443757\n",
      "Precision (BERT)    0.845265\n",
      "Recall (BERT)       0.841503\n",
      "F1 Score (BERT)     0.843027\n",
      "BLEU                0.038251\n",
      "ROUGE-1             0.288282\n",
      "ROUGE-2             0.087102\n",
      "ROUGE-L             0.273315\n",
      "METEOR              0.233272\n",
      "dtype: float64\n",
      "Precision (BERT)    0.807320\n",
      "Recall (BERT)       0.831882\n",
      "F1 Score (BERT)     0.818700\n",
      "BLEU                0.030160\n",
      "ROUGE-1             0.239315\n",
      "ROUGE-2             0.079027\n",
      "ROUGE-L             0.225743\n",
      "METEOR              0.210452\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"overall mean discharge_instructions: \" + str(overall_scores_brief_hospital_course))\n",
    "print(mean_scores_discharge_instructions)\n",
    "print(\"overall mean brief_hospital_course: \" + str(overall_scores_brief_hospital_course))\n",
    "print(mean_scores_brief_hospital_course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0ae8847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (BERT)    0.838591\n",
      "Recall (BERT)       0.838805\n",
      "F1 Score (BERT)     0.838488\n",
      "BLEU                0.041435\n",
      "ROUGE-1             0.275827\n",
      "ROUGE-2             0.082898\n",
      "ROUGE-L             0.263232\n",
      "METEOR              0.224325\n",
      "dtype: float64\n",
      "overall mean discharge instructions: 0.28770092599958014\n",
      "Precision (BERT)    0.804995\n",
      "Recall (BERT)       0.839190\n",
      "F1 Score (BERT)     0.821507\n",
      "BLEU                0.029939\n",
      "ROUGE-1             0.232463\n",
      "ROUGE-2             0.073925\n",
      "ROUGE-L             0.219769\n",
      "METEOR              0.247535\n",
      "dtype: float64\n",
      "overall mean brief hospital course: 0.2708562645384736\n"
     ]
    }
   ],
   "source": [
    "print(mean_scores_discharge_instructions)\n",
    "print(\"overall mean discharge instructions: \" + str(overall_scores_discharge_instructions))\n",
    "print(mean_scores_brief_hospital_course)\n",
    "print(\"overall mean brief hospital course: \" + str(overall_scores_brief_hospital_course))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "541fa98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision (BERT)</th>\n",
       "      <th>Recall (BERT)</th>\n",
       "      <th>F1 Score (BERT)</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.802027</td>\n",
       "      <td>0.853258</td>\n",
       "      <td>0.826850</td>\n",
       "      <td>4.774451e-02</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.087824</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.832282</td>\n",
       "      <td>0.830727</td>\n",
       "      <td>0.831504</td>\n",
       "      <td>2.216288e-79</td>\n",
       "      <td>0.207254</td>\n",
       "      <td>0.043636</td>\n",
       "      <td>0.196891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.795514</td>\n",
       "      <td>0.838217</td>\n",
       "      <td>0.816307</td>\n",
       "      <td>9.313128e-02</td>\n",
       "      <td>0.267003</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.256927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.846126</td>\n",
       "      <td>0.809325</td>\n",
       "      <td>0.827316</td>\n",
       "      <td>1.572443e-02</td>\n",
       "      <td>0.271605</td>\n",
       "      <td>0.150235</td>\n",
       "      <td>0.259259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.775890</td>\n",
       "      <td>0.845338</td>\n",
       "      <td>0.809126</td>\n",
       "      <td>6.927265e-79</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.035443</td>\n",
       "      <td>0.196364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision (BERT)  Recall (BERT)  F1 Score (BERT)          BLEU   ROUGE-1  \\\n",
       "0          0.802027       0.853258         0.826850  4.774451e-02  0.279070   \n",
       "1          0.832282       0.830727         0.831504  2.216288e-79  0.207254   \n",
       "2          0.795514       0.838217         0.816307  9.313128e-02  0.267003   \n",
       "3          0.846126       0.809325         0.827316  1.572443e-02  0.271605   \n",
       "4          0.775890       0.845338         0.809126  6.927265e-79  0.218182   \n",
       "\n",
       "    ROUGE-2   ROUGE-L  \n",
       "0  0.087824  0.250000  \n",
       "1  0.043636  0.196891  \n",
       "2  0.131148  0.256927  \n",
       "3  0.150235  0.259259  \n",
       "4  0.035443  0.196364  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_brief_hosptial_course.head()\n",
    "# scores_dishcarge_instructions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c38ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your reference and hypothesis texts here\n",
    "refs_text = \"\"\"\n",
    "Ms. ___, \n",
    ".\n",
    "You were admitted to the hospital with an infection of the skin \n",
    "on the left side of your face.  The infection did not affect \n",
    "your eye.  We gave you antibiotics for your infection and it \n",
    "improved.  You should continue augmentin by mouth for 3 days.  \n",
    ".\n",
    "During your admission, you were also noted to have an elevated \n",
    "INR.  Your coumadin was held.  Unfortunately, your INR level \n",
    "trended too low, and you were started on a heparin drip.  You \n",
    "were discharged on your home coumadin once your level was \n",
    "appropriate for your mechanical mitral valve.  \n",
    ".\n",
    "Weigh yourself every morning, call MD if weight goes up more \n",
    "than 3 lbs.\n",
    ".\n",
    "Please follow up with your primary care physician and Dr. ___ \n",
    "___ surgeon) as below.  \n",
    ".\n",
    "MEDICATIONS CHANGED THIS ADMSSION\n",
    "START augmentin 875 mg by mouth twice a day for 3 days \n",
    "TAKE ___ dose of home coumadin (2.5 mg) on evening of discharge, \n",
    "then resume normal coumadin dose\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "hyps_text = \"\"\"\n",
    "Dear patient,\n",
    "\n",
    "I am writing to inform you about the details of your hospital stay. You were admitted to the hospital because of left-sided facial swelling and pain. You were diagnosed with buccal cellulitis with a possible dental source. Your INR was 7.2, and you were given Unasyn for treatment. \n",
    "\n",
    "During your hospital stay, you were prescribed the following medications: \n",
    "\n",
    "1. metoprolol tartrate 50 mg Tablet Sig: One (1) Tablet PO BID (2 times a day). \n",
    "2. alendronate 70 mg Tablet Sig: One (1) Tablet PO once a week. \n",
    "3. polyethylene glycol 3350 17 gram Powder in Packet Sig: One (1) Powder in Packet PO DAILY (Daily) as needed for constipation. \n",
    "4. simvastatin 20 mg Tablet Sig: One (1) Tablet PO once a day. \n",
    "5. aspirin 81 mg Tablet, Chewable Sig: One (1) Tablet, Chewable PO DAILY (Daily). \n",
    "6. Vitamin D3  Oral\n",
    "7. lisinopril 5 mg Tablet Sig: One (1) Tablet PO once a day. \n",
    "8. warfarin 5 mg Tablet Sig: One (1) Tablet PO once a day: take 2.5mg ___ tablet) on evening of discharge, then resume home dose. \n",
    "9. amoxicillin-pot clavulanate 875-125 mg Tablet Sig: One (1) Tablet PO Q12H (every 12 hours) for 3.5 days.\n",
    "\n",
    "You will need to follow up with your primary care physician and dentist as an outpatient to prepare for mouth clearance surgery. You will also need to return to the clinic two days following discharge for an INR check. \n",
    "\n",
    "Please follow your medication regimen as prescribed and contact your healthcare provider if you have any concerns or questions. \n",
    "\n",
    "Best wishes for a speedy recovery!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess texts\n",
    "refs = [preprocess_text(refs_text)]\n",
    "hyps = [preprocess_text(hyps_text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dce6fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_text = refs_df['discharge_instructions'][1]    \n",
    "\n",
    "hyps_text = hyps_df['discharge_instructions'][1]\n",
    "\n",
    "# Preprocess texts\n",
    "refs = [preprocess_text(refs_text)]\n",
    "hyps = [preprocess_text(hyps_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c976c6f6-a650-4d10-83f5-8d00ef590298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        list\n",
      "\u001b[1;31mString form:\u001b[0m [\"Discharge Instructions ---\\nDear ,\\n\\nIt was a pleasure to take care of you during your recent  <...> ase call our office (555-555-5555) or 911 for immediate assistance.\\n\\nSincerely,\\nYour Team---\"]\n",
      "\u001b[1;31mLength:\u001b[0m      1\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "Built-in mutable sequence.\n",
      "\n",
      "If no argument is given, the constructor creates a new empty list.\n",
      "The argument must be an iterable if specified."
     ]
    }
   ],
   "source": [
    "refs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7759a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 500.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.20 seconds, 0.83 sentences/sec\n",
      "BLEU Score: 0.020650183521959388\n",
      "ROUGE Scores:\n",
      " rouge1: 0.45637583892617456\n",
      " rouge2: 0.12837837837837837\n",
      " rougeL: 0.2550335570469799\n",
      "BERTScore (F1): 0.8440862894058228\n",
      "METEOR Score: 0.24374932036102426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "list_of_references = [[ref.split()] for ref in refs]\n",
    "hypotheses = [hyp.split() for hyp in hyps]\n",
    "bleu_score = corpus_bleu(list_of_references, hypotheses, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "# Initialize the METEOR scorer\n",
    "meteor_scorer = evaluate.load('meteor')\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = [rouge_scorer.score(ref, hyp) for ref, hyp in zip(refs, hyps)]\n",
    "# Calculate BERTScores\n",
    "_, _, bert_scores_f1 = bert_score(refs, hyps, lang=\"en\", verbose=True)\n",
    "# Calculate METEOR scores\n",
    "meteor_scores = [meteor_scorer.compute(predictions=[hyp], references=[ref])['meteor'] for ref, hyp in zip(refs, hyps)]\n",
    "# # Calculate BLEU score\n",
    "# bleu_score = calculate_bleu(refs, hyps)\n",
    "\n",
    "# Convert BERTScores' F1 scores from tensors to NumPy arrays and calculate the mean\n",
    "bert_scores_f1_mean = torch.mean(bert_scores_f1).item()\n",
    "\n",
    "# Print scores\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "print(\"ROUGE Scores:\")\n",
    "for rouge_type in ['rouge1', 'rouge2', 'rougeL']:\n",
    "    mean_score = np.mean([score[rouge_type].fmeasure for score in rouge_scores])\n",
    "    print(f\" {rouge_type}: {mean_score}\")\n",
    "print(\"BERTScore (F1):\", bert_scores_f1_mean)\n",
    "print(\"METEOR Score:\", np.mean(meteor_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54bfda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b598fd-ad60-4184-934a-71e4150e30ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.93k/6.93k [00:00<?, ?B/s]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\s.wendelken\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "tokenizer_config.json: 100%|██████████| 25.0/25.0 [00:00<?, ?B/s]\n",
      "C:\\Users\\s.wendelken\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\s.wendelken\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config.json: 100%|██████████| 482/482 [00:00<00:00, 961kB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 7.33MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 44.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 25.0MB/s]\n",
      "model.safetensors: 100%|██████████| 1.42G/1.42G [00:27<00:00, 52.5MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 50.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.92 seconds, 0.34 sentences/sec\n",
      "BLEU Score: 0.08612427965941073\n",
      "ROUGE Scores:\n",
      " rouge1: 0.3752969121140143\n",
      " rouge2: 0.14319809069212408\n",
      " rougeL: 0.16627078384798097\n",
      "BERTScore (F1): 0.8264833092689514\n",
      "METEOR Score: 0.3243030797985192\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "def calculate_bleu(refs, hyps):\n",
    "    list_of_references = [[ref.split()] for ref in refs]\n",
    "    hypotheses = [hyp.split() for hyp in hyps]\n",
    "    bleu_score = corpus_bleu(list_of_references, hypotheses, smoothing_function=SmoothingFunction().method1)\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "# Initialize the METEOR scorer\n",
    "meteor_scorer = evaluate.load('meteor')\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = [rouge_scorer.score(ref, hyp) for ref, hyp in zip(refs, hyps)]\n",
    "# Calculate BERTScores\n",
    "_, _, bert_scores_f1 = bert_score(refs, hyps, lang=\"en\", verbose=True)\n",
    "# Calculate METEOR scores\n",
    "meteor_scores = [meteor_scorer.compute(predictions=[hyp], references=[ref])['meteor'] for ref, hyp in zip(refs, hyps)]\n",
    "# Calculate BLEU score\n",
    "bleu_score = calculate_bleu(refs, hyps)\n",
    "\n",
    "# Convert BERTScores' F1 scores from tensors to NumPy arrays and calculate the mean\n",
    "bert_scores_f1_mean = torch.mean(bert_scores_f1).item()\n",
    "\n",
    "# Print scores\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "print(\"ROUGE Scores:\")\n",
    "for rouge_type in ['rouge1', 'rouge2', 'rougeL']:\n",
    "    mean_score = np.mean([score[rouge_type].fmeasure for score in rouge_scores])\n",
    "    print(f\" {rouge_type}: {mean_score}\")\n",
    "print(\"BERTScore (F1):\", bert_scores_f1_mean)\n",
    "print(\"METEOR Score:\", np.mean(meteor_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1eefc-2931-4547-a489-1731ecec64a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87109035-2126-4d6e-835c-9e6325a826ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba86b18-579d-407b-9e4f-73e90e5337a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018f0c4-fd26-47e5-a0e1-833d4e33ffe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e674a3-24a9-412f-8df3-72eec15f1ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898aa576-abaf-4688-9f02-9e59cbc4dbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842bc23a-5e3c-44f4-920c-d99dabc416fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b49965-862c-4e3d-b1ef-adb6b3ad6dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c27105-6cd1-405a-bc97-e1b4e37d9c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0fb3b-3855-41dc-a51e-f4c801dc5990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
