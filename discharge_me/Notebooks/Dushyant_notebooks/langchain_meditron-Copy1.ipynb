{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9378b-aa4a-480f-a08a-4a0177fa793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-community langchain-openai langchain-chroma langchain langchain-huggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55152de9-756b-4bfd-b609-01a36875c8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 17:08:17,935\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader,TextLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from vllm import LLM\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21c6cf3-4d03-426e-bd0c-a51bfae67a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/\"\n",
    "loader = DirectoryLoader(path, glob=\"*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a45b71-ceb3-4307-a887-db63c2289818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89e01cf4-3c63-49a6-98e0-13a7b39058c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62642d0b-2942-4aa1-aa54-3d5079b83e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import GOOGLE_API_KEY\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6753bbbc-d80c-42fe-a43e-2fb5cf53a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(retriever, query):\n",
    "\n",
    "    try:\n",
    "        # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "        llm = LLM(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "        # llm = ChatOpenAI(model=\"gpt-4\")\n",
    "        # llm =chat\n",
    "#         llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"epfl-llm/meditron-7b\",\n",
    "#     task=\"text-generation\",\n",
    "# )\n",
    "        template = \"\"\"\n",
    "            You are a helpful Medical AI assistant. Answer the question based on the context provided. Respond only if the answer is in the context.\n",
    "            If the answer is not in the context then respond, Cannot answer the question.\n",
    "            Stick to the context while answering and be concise.\n",
    "            context: {context}\n",
    "            question: {input}\n",
    "            answer:\n",
    "            \"\"\"\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "        combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "        retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "        response = retrieval_chain.invoke({\"input\": query})\n",
    "        # print(response[\"answer\"])\n",
    "        return response[\"answer\"]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating the response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2a9df09-b6fe-44e6-8979-02a8446abf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "\"Given the History of Present Illness, What is the relevant past medical history?\",\n",
    "\"Given the HPI, chief complaint, and initial ICD diagnosis, Why was the patient admitted to the hospital? What symptoms, medical condition, or other reason caused the patient to come to or be brought into the hospital?\",\n",
    "\"Based on the admission medication list, what other existing or ongoing medical conditions did the patient have?\",\n",
    "\"Given the history of present illness, admission and discharge diagnosis, What medical conditions or symptoms was this patient treated for in the hospital?\",\n",
    "\"given the history of present illness, What was the initial treatment course?\",\n",
    "\"Given the History of Present illness, What was the initial diagnostic work up and pertinent results?\",\n",
    "\"Given the radiology notes, What radiology, imaging or other studies that were performed and for what inducation? Summarize the impressions from each study. Make a table of Imaging study, indication, and impression.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e251756-7ff2-4596-a6df-9b5e76f62aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 16:59:47 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 17:00:20 model_runner.py:160] Loading model weights took 14.9595 GB\n",
      "INFO 07-03 17:00:22 gpu_executor.py:83] # GPU blocks: 27961, # CPU blocks: 2048\n",
      "INFO 07-03 17:00:23 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-03 17:00:23 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-03 17:00:57 model_runner.py:965] Graph capturing finished in 34 secs.\n",
      "An error occurred while generating the response: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'vllm.entrypoints.llm.LLM'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(retriever, questions[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f206b895-963b-48ff-8483-282c50651ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 17:01:04 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while generating the response: CUDA out of memory. Tried to allocate 224.00 MiB. GPU \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(retriever, questions[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8baeaab3-6239-4910-b2c5-51bc9087cb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the History of Present Illness, What is the relevant past medical history? \n",
      "\n",
      "- Bipolar  \n",
      "- Hepatitis C  \n",
      "- HLD  \n",
      "- Diabetes, steroid-induced  \n",
      "- Self-reported bullous pemphigoid, but no clinic or laboratory evidence to support this diagnosis\n",
      "- COPD  \n",
      "- Cholelithiasis  \n",
      "- GERD  \n",
      "- Aortic aneurysm  \n",
      "- Osteoporosis\n",
      "\n",
      "*********************\n",
      "Given the HPI, chief complaint, and initial ICD diagnosis, Why was the patient admitted to the hospital? What symptoms, medical condition, or other reason caused the patient to come to or be brought into the hospital? \n",
      "\n",
      "The patient was admitted to the hospital due to leg swelling, abdominal distention, and complaints of oral ulcers. The patient was experiencing significant leg pain and had a history of pemphigoid, a skin condition. The patient also reported that his abdomen was swollen. His medical condition and symptoms, along with a primary discharge diagnosis of cellulitis of the left lower limb, prompted his admission to the hospital.\n",
      "\n",
      "*********************\n",
      "Based on the admission medication list, what other existing or ongoing medical conditions did the patient have? \n",
      "\n",
      "The patient had conditions such as asthma or COPD (medicated with Albuterol and Fluticasone-Salmeterol), pain (medicated with OxyCODONE and Morphine), anxiety (medicated with ClonazePAM), insomnia (medicated with Amitriptyline), inflammation (medicated with PredniSONE), and potential cardiovascular issues (medicated with Aspirin).\n",
      "\n",
      "*********************\n",
      "Given the history of present illness, admission and discharge diagnosis, What medical conditions or symptoms was this patient treated for in the hospital? \n",
      "\n",
      "The patient was treated for leg swelling, oral ulcers, and abdominal swelling. The primary discharge diagnosis was cellulitis.\n",
      "\n",
      "*********************\n",
      "given the history of present illness, What was the initial treatment course? \n",
      "\n",
      "The patient was given Duonebs and CTX 1g IV for LLE cellulitis.\n",
      "\n",
      "*********************\n",
      "Given the History of Present illness, What was the initial diagnostic work up and pertinent results? \n",
      "\n",
      "The initial diagnostic work-up included labs and imaging. Lab results were notable for a White Blood Cell (WBC) count of 12.5. Imaging included a Lower Left Extremity Ultrasound (LLE US) which did not show evidence of Deep Vein Thrombosis (DVT) and a Chest X-Ray (CXR) that showed mild cardiomegaly and mild vascular congestion. A bedside ultrasound did not show any evidence of ascites.\n",
      "\n",
      "*********************\n",
      "Given the radiology notes, What radiology, imaging or other studies that were performed and for what inducation? Summarize the impressions from each study. Make a table of Imaging study, indication, and impression. \n",
      "\n",
      "| Imaging Study | Indication | Impression |\n",
      "| --- | --- | --- |\n",
      "| Chest PA and lateral | Leg swelling, evaluate for acute process. | Mild cardiomegaly and mild pulmonary vascular congestion. Left lower lobe atelectasis has improved. |\n",
      "| Unilateral Lower Extremity Veins Left | Leg swelling, evaluate for DVT. | No evidence of deep venous thrombosis in the left lower extremity veins. Nonvisualization of the left peroneal veins. |\n",
      "\n",
      "*********************\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    response = generate_response(retriever, question)\n",
    "    print(question,\"\\n\")\n",
    "    print(response)\n",
    "    print(\"\\n*********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79e926f5-d792-49ec-baf6-8dbb80179a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.26s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while generating the response: 'NoneType' object cannot be interpreted as an integer\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "def initialize_llama_model(model_id):\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        device=0,\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "def generate_response(retriever, query, llama_pipeline):\n",
    "    try:\n",
    "        template = \"\"\"\n",
    "            You are a helpful Medical AI assistant. Answer the question based on the context provided. Respond only if the answer is in the context.\n",
    "            If the answer is not in the context then respond, Cannot answer the question.\n",
    "            Stick to the context while answering and be concise.\n",
    "            context: {context}\n",
    "            question: {input}\n",
    "            answer:\n",
    "            \"\"\"\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "        combine_docs_chain = create_stuff_documents_chain(llama_pipeline, prompt)\n",
    "        retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "        \n",
    "        # Prepare the messages for the LLaMA model\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful Medical AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ]\n",
    "        \n",
    "        terminators = [\n",
    "            llama_pipeline.tokenizer.eos_token_id,\n",
    "            llama_pipeline.tokenizer.convert_tokens_to_ids(\"\")\n",
    "        ]\n",
    "\n",
    "        outputs = llama_pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=256,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        \n",
    "        # Extract the generated text from the output\n",
    "        generated_text = outputs[0][\"generated_text\"][-1]\n",
    "        \n",
    "        # Assuming the answer is directly in the generated text\n",
    "        return generated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating the response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize the LLaMA model\n",
    "model_id = \"/work/AI-EHR/discharge_me/Notebooks/Dushyant_notebooks/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llama_pipeline = initialize_llama_model(model_id)\n",
    "\n",
    "# Example usage\n",
    "query = \"What are the symptoms of COVID-19?\"\n",
    "response = generate_response(retriever, query, llama_pipeline)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32287bb-e20e-43ab-a1b1-0862e180c68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
