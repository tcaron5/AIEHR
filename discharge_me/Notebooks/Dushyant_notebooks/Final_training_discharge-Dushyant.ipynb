{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "172c1648-ede7-47e3-9d6d-db265976cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
      "To initialize your shell, run\n",
      "\n",
      "    $ conda init <SHELL_NAME>\n",
      "\n",
      "Currently supported shells are:\n",
      "  - bash\n",
      "  - fish\n",
      "  - tcsh\n",
      "  - xonsh\n",
      "  - zsh\n",
      "  - powershell\n",
      "\n",
      "See 'conda init --help' for more information and options.\n",
      "\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9027dcf0-c75d-4392-9a6d-03d13a4bed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv\n",
    "# path for input and target data tables\n",
    "\n",
    "diagnosis_path = '../data/test_phase_2/diagnosis_hadm.csv'\n",
    "discharge_path ='../data/test_phase_2/discharge.csv'\n",
    "edstays_path = '../data/test_phase_2/edstays.csv'\n",
    "radiology_path = '../data/test_phase_2/radiology.csv'\n",
    "triage_path = '../data/test_phase_2/triage.csv'\n",
    "target_path = '../data/test_phase_2/discharge_target.csv'\n",
    "discharge_sections_path = '../data/test_phase_2/discharge_sections.csv'\n",
    "radiology_sections_path = '../data/test_phase_2/radiology_sections.csv'\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# read data\n",
    "diagnosis_df = pd.read_csv(diagnosis_path, keep_default_na=False)\n",
    "discharge_df = pd.read_csv(discharge_path, keep_default_na=False)\n",
    "edstays_df = pd.read_csv(edstays_path, keep_default_na=False)\n",
    "radiology_df = pd.read_csv(radiology_path, keep_default_na=False)\n",
    "triage_df = pd.read_csv(triage_path, keep_default_na=False)\n",
    "target_df = pd.read_csv(target_path, keep_default_na=False)\n",
    "\n",
    "discharge_sections_df = pd.read_csv(discharge_sections_path, keep_default_na=False)\n",
    "radiology_sections_df = pd.read_csv(radiology_sections_path, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a3c8e86-7907-405b-9f40-0bbf5e2062a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-26 16:36:32 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahajan.d/.conda/envs/ai-ehr/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the LLM object with the specified model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m LLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model = LLM(model=\"meta-llama/Meta-Llama-3-8B\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/vllm/entrypoints/llm.py:144\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    124\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    125\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    126\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m LLMEngine\u001b[38;5;241m.\u001b[39mfrom_engine_args(\n\u001b[1;32m    145\u001b[0m     engine_args, usage_context\u001b[38;5;241m=\u001b[39mUsageContext\u001b[38;5;241m.\u001b[39mLLM_CLASS)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/vllm/engine/llm_engine.py:363\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    360\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_config\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m    365\u001b[0m     executor_class\u001b[38;5;241m=\u001b[39mexecutor_class,\n\u001b[1;32m    366\u001b[0m     log_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m engine_args\u001b[38;5;241m.\u001b[39mdisable_log_stats,\n\u001b[1;32m    367\u001b[0m     usage_context\u001b[38;5;241m=\u001b[39musage_context,\n\u001b[1;32m    368\u001b[0m )\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/vllm/engine/llm_engine.py:223\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config, decoding_config, executor_class, log_stats, usage_context)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config_fields \u001b[38;5;241m=\u001b[39m _load_generation_config_dict(\n\u001b[1;32m    221\u001b[0m     model_config)\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(\n\u001b[1;32m    224\u001b[0m     model_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[1;32m    225\u001b[0m     cache_config\u001b[38;5;241m=\u001b[39mcache_config,\n\u001b[1;32m    226\u001b[0m     parallel_config\u001b[38;5;241m=\u001b[39mparallel_config,\n\u001b[1;32m    227\u001b[0m     scheduler_config\u001b[38;5;241m=\u001b[39mscheduler_config,\n\u001b[1;32m    228\u001b[0m     device_config\u001b[38;5;241m=\u001b[39mdevice_config,\n\u001b[1;32m    229\u001b[0m     lora_config\u001b[38;5;241m=\u001b[39mlora_config,\n\u001b[1;32m    230\u001b[0m     vision_language_config\u001b[38;5;241m=\u001b[39mvision_language_config,\n\u001b[1;32m    231\u001b[0m     speculative_config\u001b[38;5;241m=\u001b[39mspeculative_config,\n\u001b[1;32m    232\u001b[0m     load_config\u001b[38;5;241m=\u001b[39mload_config,\n\u001b[1;32m    233\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/vllm/executor/executor_base.py:41\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_language_config \u001b[38;5;241m=\u001b[39m vision_language_config\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeculative_config \u001b[38;5;241m=\u001b[39m speculative_config\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_executor()\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:22\u001b[0m, in \u001b[0;36mGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the worker and load the model.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPUExecutor only supports single GPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39minit_device()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39mload_model()\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:67\u001b[0m, in \u001b[0;36mGPUExecutor._create_worker\u001b[0;34m(self, local_rank, rank, distributed_init_method)\u001b[0m\n\u001b[1;32m     61\u001b[0m     worker_class_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_spec_worker\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m wrapper \u001b[38;5;241m=\u001b[39m WorkerWrapperBase(\n\u001b[1;32m     64\u001b[0m     worker_module_name\u001b[38;5;241m=\u001b[39mworker_module_name,\n\u001b[1;32m     65\u001b[0m     worker_class_name\u001b[38;5;241m=\u001b[39mworker_class_name,\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m wrapper\u001b[38;5;241m.\u001b[39minit_worker(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_worker_kwargs(local_rank, rank,\n\u001b[1;32m     68\u001b[0m                                               distributed_init_method))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\u001b[38;5;241m.\u001b[39mworker\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/vllm/worker/worker_base.py:134\u001b[0m, in \u001b[0;36mWorkerWrapperBase.init_worker\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m mod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_module_name)\n\u001b[1;32m    133\u001b[0m worker_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_class_name)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker \u001b[38;5;241m=\u001b[39m worker_class(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/vllm/worker/worker.py:75\u001b[0m, in \u001b[0;36mWorker.__init__\u001b[0;34m(self, model_config, parallel_config, scheduler_config, device_config, cache_config, load_config, local_rank, rank, distributed_init_method, lora_config, vision_language_config, speculative_config, is_driver_worker)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config, (\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be tested: vision language model with LoRA settings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m ModelRunnerClass \u001b[38;5;241m=\u001b[39m (EmbeddingModelRunner \u001b[38;5;28;01mif\u001b[39;00m\n\u001b[1;32m     74\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode \u001b[38;5;28;01melse\u001b[39;00m ModelRunner)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_runner \u001b[38;5;241m=\u001b[39m ModelRunnerClass(\n\u001b[1;32m     76\u001b[0m     model_config,\n\u001b[1;32m     77\u001b[0m     parallel_config,\n\u001b[1;32m     78\u001b[0m     scheduler_config,\n\u001b[1;32m     79\u001b[0m     device_config,\n\u001b[1;32m     80\u001b[0m     cache_config,\n\u001b[1;32m     81\u001b[0m     load_config\u001b[38;5;241m=\u001b[39mload_config,\n\u001b[1;32m     82\u001b[0m     lora_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config,\n\u001b[1;32m     83\u001b[0m     kv_cache_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mcache_dtype,\n\u001b[1;32m     84\u001b[0m     is_driver_worker\u001b[38;5;241m=\u001b[39mis_driver_worker,\n\u001b[1;32m     85\u001b[0m     vision_language_config\u001b[38;5;241m=\u001b[39mvision_language_config,\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Uninitialized cache engine. Will be initialized by\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# initialize_cache.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine: CacheEngine\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/vllm/worker/model_runner.py:119\u001b[0m, in \u001b[0;36mModelRunner.__init__\u001b[0;34m(self, model_config, parallel_config, scheduler_config, device_config, cache_config, load_config, lora_config, kv_cache_dtype, is_driver_worker, vision_language_config)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# When using CUDA graph, the input block tables must be padded to\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# max_seq_len_to_capture. However, creating the block table in\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Python can be expensive. To optimize this, we cache the block table\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# in numpy and only copy the actual input content at every iteration.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# The shape of the cached block table will be\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# (max batch size to capture, max context len to capture / block size).\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_block_tables \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    117\u001b[0m     (\u001b[38;5;28mmax\u001b[39m(_BATCH_SIZES_TO_CAPTURE), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_max_block_per_batch()),\n\u001b[1;32m    118\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_backend \u001b[38;5;241m=\u001b[39m get_attn_backend(\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget_num_attention_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config),\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget_head_size(),\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget_num_kv_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config),\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget_sliding_window(),\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache_dtype,\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size,\n\u001b[1;32m    127\u001b[0m )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Create processor for multi-modal data\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_language_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/vllm/attention/selector.py:43\u001b[0m, in \u001b[0;36mget_attn_backend\u001b[0;34m(num_heads, head_size, num_kv_heads, sliding_window, dtype, kv_cache_dtype, block_size, is_blocksparse)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocksparse_attn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     40\u001b[0m         BlocksparseFlashAttentionBackend)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BlocksparseFlashAttentionBackend\n\u001b[0;32m---> 43\u001b[0m backend \u001b[38;5;241m=\u001b[39m which_attn_to_use(num_heads, head_size, num_kv_heads,\n\u001b[1;32m     44\u001b[0m                             sliding_window, dtype, kv_cache_dtype,\n\u001b[1;32m     45\u001b[0m                             block_size)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m _Backend\u001b[38;5;241m.\u001b[39mFLASH_ATTN:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         FlashAttentionBackend)\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/vllm/attention/selector.py:129\u001b[0m, in \u001b[0;36mwhich_attn_to_use\u001b[0;34m(num_heads, head_size, num_kv_heads, sliding_window, dtype, kv_cache_dtype, block_size)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# FlashAttn in NVIDIA GPUs.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_backend \u001b[38;5;241m==\u001b[39m _Backend\u001b[38;5;241m.\u001b[39mFLASH_ATTN:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_capability()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;66;03m# Volta and Turing NVIDIA GPUs.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use FlashAttention-2 backend for Volta and Turing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPUs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m         selected_backend \u001b[38;5;241m=\u001b[39m _Backend\u001b[38;5;241m.\u001b[39mXFORMERS\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/torch/cuda/__init__.py:430\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_capability\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     prop \u001b[38;5;241m=\u001b[39m get_device_properties(device)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/torch/cuda/__init__.py:444\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     _lazy_init()  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/.conda/envs/ai-ehr/lib/python3.11/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_init()\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the LLM object with the specified model\n",
    "model = LLM(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# model = LLM(model=\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')\n",
    "# Imports\n",
    "import torch\n",
    "# generate_prompt function\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.  # noqa: E501\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ea3aa0b-42a2-42b9-ba49-3775eb22b572",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72aba9f3-4c23-4f58-ae13-1027dc68e627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mSamplingParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbest_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_beam_search\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlength_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstop_token_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minclude_stop_str_in_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_eos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlogprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprompt_logprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdetokenize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mspaces_between_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlogits_processors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtruncate_prompt_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFieldInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequired\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Sampling parameters for text generation.\n",
       "\n",
       "Overall, we follow the sampling parameters from the OpenAI text completion\n",
       "API (https://platform.openai.com/docs/api-reference/completions/create).\n",
       "In addition, we support beam search, which is not supported by OpenAI.\n",
       "\n",
       "Args:\n",
       "    n: Number of output sequences to return for the given prompt.\n",
       "    best_of: Number of output sequences that are generated from the prompt.\n",
       "        From these `best_of` sequences, the top `n` sequences are returned.\n",
       "        `best_of` must be greater than or equal to `n`. This is treated as\n",
       "        the beam width when `use_beam_search` is True. By default, `best_of`\n",
       "        is set to `n`.\n",
       "    presence_penalty: Float that penalizes new tokens based on whether they\n",
       "        appear in the generated text so far. Values > 0 encourage the model\n",
       "        to use new tokens, while values < 0 encourage the model to repeat\n",
       "        tokens.\n",
       "    frequency_penalty: Float that penalizes new tokens based on their\n",
       "        frequency in the generated text so far. Values > 0 encourage the\n",
       "        model to use new tokens, while values < 0 encourage the model to\n",
       "        repeat tokens.\n",
       "    repetition_penalty: Float that penalizes new tokens based on whether\n",
       "        they appear in the prompt and the generated text so far. Values > 1\n",
       "        encourage the model to use new tokens, while values < 1 encourage\n",
       "        the model to repeat tokens.\n",
       "    temperature: Float that controls the randomness of the sampling. Lower\n",
       "        values make the model more deterministic, while higher values make\n",
       "        the model more random. Zero means greedy sampling.\n",
       "    top_p: Float that controls the cumulative probability of the top tokens\n",
       "        to consider. Must be in (0, 1]. Set to 1 to consider all tokens.\n",
       "    top_k: Integer that controls the number of top tokens to consider. Set\n",
       "        to -1 to consider all tokens.\n",
       "    min_p: Float that represents the minimum probability for a token to be\n",
       "        considered, relative to the probability of the most likely token.\n",
       "        Must be in [0, 1]. Set to 0 to disable this.\n",
       "    seed: Random seed to use for the generation.\n",
       "    use_beam_search: Whether to use beam search instead of sampling.\n",
       "    length_penalty: Float that penalizes sequences based on their length.\n",
       "        Used in beam search.\n",
       "    early_stopping: Controls the stopping condition for beam search. It\n",
       "        accepts the following values: `True`, where the generation stops as\n",
       "        soon as there are `best_of` complete candidates; `False`, where an\n",
       "        heuristic is applied and the generation stops when is it very\n",
       "        unlikely to find better candidates; `\"never\"`, where the beam search\n",
       "        procedure only stops when there cannot be better candidates\n",
       "        (canonical beam search algorithm).\n",
       "    stop: List of strings that stop the generation when they are generated.\n",
       "        The returned output will not contain the stop strings.\n",
       "    stop_token_ids: List of tokens that stop the generation when they are\n",
       "        generated. The returned output will contain the stop tokens unless\n",
       "        the stop tokens are special tokens.\n",
       "    include_stop_str_in_output: Whether to include the stop strings in\n",
       "        output text. Defaults to False.\n",
       "    ignore_eos: Whether to ignore the EOS token and continue generating\n",
       "        tokens after the EOS token is generated.\n",
       "    max_tokens: Maximum number of tokens to generate per output sequence.\n",
       "    min_tokens: Minimum number of tokens to generate per output sequence\n",
       "        before EOS or stop_token_ids can be generated\n",
       "    logprobs: Number of log probabilities to return per output token.\n",
       "        Note that the implementation follows the OpenAI API: The return\n",
       "        result includes the log probabilities on the `logprobs` most likely\n",
       "        tokens, as well the chosen tokens. The API will always return the\n",
       "        log probability of the sampled token, so there  may be up to\n",
       "        `logprobs+1` elements in the response.\n",
       "    prompt_logprobs: Number of log probabilities to return per prompt token.\n",
       "    detokenize: Whether to detokenize the output. Defaults to True.\n",
       "    skip_special_tokens: Whether to skip special tokens in the output.\n",
       "    spaces_between_special_tokens: Whether to add spaces between special\n",
       "        tokens in the output.  Defaults to True.\n",
       "    logits_processors: List of functions that modify logits based on\n",
       "        previously generated tokens.\n",
       "    truncate_prompt_tokens: If set to an integer k, will use only the last k\n",
       "        tokens from the prompt (i.e., left truncation). Defaults to None\n",
       "        (i.e., no truncation).\n",
       "\u001b[0;31mFile:\u001b[0m           ~/discharge_me/lib/python3.10/site-packages/vllm/sampling_params.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SamplingParams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab07ee34-ff1f-48fa-ada2-6ad080e1da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_model(instruction, context_input, model):\n",
    "    # Generate the prompt using the instruction and context\n",
    "    prompt = generate_prompt(instruction, context_input)\n",
    "\n",
    "    # Setup sampling parameters for generation\n",
    "    # Here we only calculate length to control the output length dynamically\n",
    "    context_length = len(tokenizer(prompt, return_tensors=\"pt\").input_ids[0])\n",
    "    print(context_length)\n",
    "    stop_str = \"--- end of response\"\n",
    "    max_input_length = min(2048, tokenizer.model_max_length - context_length)\n",
    "    sampling_params = SamplingParams(temperature=0.01, max_tokens=max_input_length, stop=stop_str,)\n",
    "\n",
    "    # Generate the response using the model\n",
    "    outputs = model.generate(prompt, sampling_params)\n",
    "    response_text = outputs[0].outputs[0].text  # Adjust based on actual output structure\n",
    "\n",
    "    # outputs = llm.generate(final_prompt, sampling_params)\n",
    "    # answer_texts = [output.outputs[0].text for output in outputs]\n",
    "    \n",
    "    # # Extract the \"Response\" part from the generated text\n",
    "    # response = response_text.split(\"### Response:\")[1].strip()\n",
    "    # response = response_text.split(\"--- end of response\")[1].strip() \n",
    "    \n",
    "    # Print and return the response\n",
    "    print(response_text)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4427ae2a-5456-4e1c-8c12-fe53d7962330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract HPI summary USE THIS\n",
    "def generate_bhc(hadm_id):\n",
    "    # topic = '''\n",
    "    # The topic is about clinical notes, medical records, and other text documents from electronic health records (EHR) \n",
    "    # from a patient's hospital admission. \n",
    "    # You will be provided with input text and data from a specific patient's medical records and clinical notes\n",
    "    # delimited by xml tags <text> </text>. Use only this data to answer questions about the EHR.  \n",
    "    # Answer clearly and concisely.\n",
    "    # '''.replace('\\n', '')\n",
    "    topic = '''\n",
    "    The topic is about clinical notes, medical records, and other text documents from electronic health records (EHR) \n",
    "    from a patient's hospital admission. \n",
    "    You will be provided with input text and data from a specific patient's medical records and clinical notes \n",
    "    delimited by xml tags <text> </text>. Use only this data to answer questions about the EHR.  \n",
    "    '''.replace('\\n', '').replace('    ', '')\n",
    "    \n",
    "    prompt = '''\\n\n",
    "    Write the text for the \"brief hospital course\" section of a discharge summary note \n",
    "    based on the input text using the following template:\n",
    "    \n",
    "    Brief Hospital Course\n",
    "    The patient with a history of [briefly describe the relevent medical history or existing medical problems] \n",
    "    presents with [briefly summarize the patient's symptoms]. They were admitted to the [what unit, service or floor were they admitted to?]\n",
    "    for [briefly summarize the reason for admission, initial diagnosis, and any major procedures or treatments].\n",
    "    [Briefly summarize the discharge diagnosis at discharge]\n",
    "    \n",
    "    Problem-based hospital course---\n",
    "    # [Medical condition 1] --- [briefly describe presentation and severity for this medical condition or symptom].\n",
    "    [Summarize the related diagnostic work up and notable results]. \n",
    "    [Summarize the related medical or surgical treatment course].\n",
    "\n",
    "    # [Medical condition 2] -- [briefly describe presentation and severity for this medical condition or symptom].\n",
    "    [Summarize the diagnostic work up and notable results related to this medical condition]. \n",
    "    [Summarize the related medical or surgical treatment course]. \n",
    "\n",
    "    --- end of response\n",
    "\n",
    "    Based on the input text, answer these questions about the patient's hospital admission:\n",
    "    What are the patient's major medical conditions?\n",
    "    Why was the patient admitted to the hospital?\n",
    "    What symptoms did the patient present with? What was the severity and duration of the presenting symptoms?\n",
    "    What treatments and procedures were administered and for what symptoms or medical conditions?\n",
    "    What unit or floor were they admitted to?\n",
    "    What was the diagnostic work up? What were the notable results?\n",
    "    What medications were used to treat the patient? Were any medications new or discontinued?\n",
    "    Are there changes to the existing medications?\n",
    "    What was the patient diagnosed with at discharge?\n",
    "    What was the patient initially diagnosed with?\n",
    "    \n",
    "    Fill in the blanks:\n",
    "    Use the answers to the questions above to fill in the blanks.\n",
    "    If you do not know the answer, fill in the blank with \"___\".\n",
    "    Answer clearly and concisely. Do not include information about the review of systems.\n",
    "    Stop generating when you reach \"--- end of response\"\n",
    "    Do not repeat the instructions in your response or make comments after your response.\n",
    "    '''.replace('    ', '')\n",
    "    \n",
    "    # Using the following input data as context, \n",
    "    # Summarize the patient's hospital course in the following way --\n",
    "    # Start with brief description of the patient's major medical conditions and why they were admitted to the hospital.\n",
    "    # Then, in a new section, outline the details of the treatment course and events of the hospital stay, \n",
    "    # focusing on the relevant diagnosis, medical conditions,\n",
    "    # diagnostic work up, relevant results, procedures, medications that were used, and changes to baseline medication therapies. \n",
    "    # Organize the outline according to the major medical conditions and diagnosis.\n",
    "    # Limit your response to 1024 words or less.\n",
    "    # '''.replace('\\n', '')\n",
    "    # prompt = '''\n",
    "    # Using the following patient specific data, \n",
    "    # briefly summarize the major medical conditions, reason for hospital admission, diagnostic workup, and treatment course.\n",
    "    # '''.replace('\\n', '')\n",
    "    \n",
    "    context_str =''\n",
    "    df = discharge_sections_df\n",
    "    cur_df = df[df['hadm_id'] == hadm_id]\n",
    "    cols = ['CC']\n",
    "    context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    context_str = f\"The chief complaint was --- {context_data} \\n\"\n",
    "    cols = ['HPI']\n",
    "    context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    context_str = context_str + f\"The History of present illness --- \\n{context_data} \\n\"\n",
    "    # cols = ['Medication Lists']\n",
    "    # context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    # context_str = context_str + f\"The admission and discharge medication lists:\\n{context_data} \\n\"\n",
    "    # cols = ['Pertinent Results']\n",
    "    # context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    # context_str = context_str + f\"The Pertinent Results were:\\n{context_data} \\n\"\n",
    "    cols = ['Major Surgical Procedure']\n",
    "    context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    context_str = context_str + f\"The major procedures were --- \\n{context_data} \\n\"\n",
    "    cols = ['Discharge Diagnosis']\n",
    "    context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    context_str = context_str + f\"At discharge, the patient was diagnosed with --- \\n{context_data} \\n\"\n",
    "    context_str = context_str.replace(\":\", '---').replace(\"==\", '')\n",
    "    \n",
    "    df = diagnosis_df\n",
    "    cols = ['icd_title']\n",
    "    cur_df = df[df['hadm_id'] == hadm_id]\n",
    "    context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n ')\n",
    "    context_str = context_str + f\"The patient was initially diagnosed with ICD Diagnosis titles --- \\n{context_data}\\n \"\n",
    "    \n",
    "    \n",
    "    instruction = f\"{topic} \\n {prompt}\"\n",
    "    context_input = f\"<text> {context_str}</text> \"\n",
    "    \n",
    "    response = generate_with_model(instruction, context_input, model)\n",
    "    return response\n",
    "    \n",
    "# # extract HPI summary USE THIS\n",
    "# def generate_bhc(hadm_id):\n",
    "#     topic = '''\n",
    "#     The topic is about clinical notes, medical records, and other text documents from electronic health records (EHR) \n",
    "#     from a patient's hospital admission. \n",
    "#     You will be provided with text and data from a specific patient's medical records and clinical notes\n",
    "#     delimited by xml tags <text> </text>. Use only this data to answer questions about the EHR.  \n",
    "#     Answer clearly and concisely.\n",
    "#     '''.replace('\\n', '')\n",
    "    \n",
    "#     # prompt = '''\n",
    "#     # Using the following patient specific data, \n",
    "#     # briefly summarize the major medical conditions, reason for hospital admission, diagnostic workup, and treatment course.\n",
    "#     # Then, create a list of the patient's acute and chronic medical conditions and the associated treatments course.\n",
    "#     # '''.replace('\\n', '')\n",
    "#     prompt = '''\n",
    "#     Using the following patient specific data, \n",
    "#     briefly summarize the major medical conditions, reason for hospital admission, diagnostic workup, and treatment course.\n",
    "#     '''.replace('\\n', '')\n",
    "    \n",
    "#     context_str =''\n",
    "#     df = discharge_sections_df\n",
    "#     cur_df = df[df['hadm_id'] == hadm_id]\n",
    "#     cols = ['CC']\n",
    "#     context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "#     context_str = f\"The chief complaint was - {context_data} \\n\"\n",
    "#     cols = ['HPI']\n",
    "#     context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "#     context_str = context_str + f\"The History of present illness: \\n{context_data} \\n\"\n",
    "#     # cols = ['Medication Lists']\n",
    "#     # context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "#     # context_str = context_str + f\"The admission and discharge medication lists:\\n{context_data} \\n\"\n",
    "#     # cols = ['Pertinent Results']\n",
    "#     # context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "#     # context_str = context_str + f\"The Pertinent Results were:\\n{context_data} \\n\"\n",
    "#     cols = ['Major Surgical Procedure']\n",
    "#     context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "#     context_str = context_str + f\"The major procedures were:\\n{context_data} \\n\"\n",
    "#     cols = ['Discharge Diagnosis']\n",
    "#     context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "#     context_str = context_str + f\"The patient was diagnosed with:\\n{context_data} \\n\"\n",
    "    \n",
    "#     df = diagnosis_df\n",
    "#     cols = ['icd_title']\n",
    "#     cur_df = df[df['hadm_id'] == hadm_id]\n",
    "#     context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n ')\n",
    "#     context_str = context_str + f\"The patient was initially diagnosed with ICD Diagnosis titles: \\n{context_data}\\n \"\n",
    "    \n",
    "    \n",
    "#     instruction = f\"{topic} \\n Instruction: {prompt}\"\n",
    "#     context_input = f\"<text> Text:{context_str}</text> \"\n",
    "    \n",
    "#     response = generate_with_model(instruction, context_input, model)\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6634c34-926e-4dce-aa48-4bca1af131d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_discharge_instructions(hadm_id):\n",
    "    # topic = '''\n",
    "    # The topic is about clinical notes, medical records, and other text documents from electronic health records (EHR) \n",
    "    # from a patient's hospital admission. \n",
    "    # You will be provided with input text and data from a specific patient's medical records and clinical notes\n",
    "    # delimited by xml tags <text> </text>. Use only this data to answer questions about the EHR.  \n",
    "    # Answer clearly and concisely.\n",
    "    # '''.replace('\\n', '')\n",
    "    \n",
    "    # prompt = '''\n",
    "    # Given the following input data from a patient's hospital stay,\n",
    "    # Compose a letter to the patient that includes answers to the following questions.\n",
    "    # What was the patient admitted for? What were the major treatments and exam findings?\n",
    "    # What is the recommended follow-up plan or instructions?\n",
    "    # Are there any changes to the patient's existing medication, \n",
    "    # and if so, what are the new medications, dose changes, or medications that should be stopped?\n",
    "    # The letter should be courteous, easy to understand, and it should be written in layman's language. \n",
    "    # Limit the use of medical jargon and acronyms.\n",
    "    # Limit your response to 500 words or less.\n",
    "    # '''.replace('\\n', '')\n",
    "\n",
    "    topic = '''\n",
    "    The topic is about clinical notes, medical records, and other text documents from electronic health records (EHR) \n",
    "    from a patient's hospital admission. \n",
    "    You will be provided with input text and data from a specific patient's medical records and clinical notes \n",
    "    delimited by xml tags <text> </text>.  \n",
    "    '''.replace('\\n', '').replace('    ', '')\n",
    "    \n",
    "    prompt = '''\\n\n",
    "    Write a brief and curteous letter to the patient that summarizes their hospital stay and communicates \n",
    "    follow-up instructions as well as important changes to their medications. Use the \n",
    "    following template:\n",
    "    \n",
    "    Discharge Instructions ---\n",
    "    Dear ____,\n",
    "\n",
    "    It was a pleasure to take care of you during your recent hospital admission. \n",
    "    You were admitted to the hospital because [explain the reason for admission]. \n",
    "    [Briefly explain the diagnostic work up and explain the results]\n",
    "    During your hospital stay your treatments included [briefly explain the major treatments and procedures].\n",
    "\n",
    "    [Briefly explain ongoing issues and follow-up recommendations].\n",
    "\n",
    "    [Briefly explain any new medications and changes to medications].\n",
    "\n",
    "    If your symptoms return, please call our office (555-555-5555) or 911 for immedicate assistance\n",
    "\n",
    "    Sincerely,\n",
    "    Your Team___\n",
    "    \n",
    "    --- end of response\n",
    "\n",
    "    Questions to answer based on the input text:\n",
    "    Why was the patient admitted to the hospital?\n",
    "    What treatments and procedures were administered and for what symptoms or medical conditions?\n",
    "    What was the diagnostic work up related to the chief complaint? What were the notable results?\n",
    "    What medications were used to treat the patient? Were any medications new or discontinued?\n",
    "    Are there changes to the existing medications?\n",
    "    What are the ongoing issues and follow-up recommendations?\n",
    "    \n",
    "    Fill in the blanks:\n",
    "    Use the input data and answers to the questions above to fill in the blanks.\n",
    "    If you do not know the answer, fill in the blank with \"___\".\n",
    "    Answer clearly and concisely. Use easy to understand language or layman's terms.\n",
    "    Address the patient as \"you\". Do not refer to the patient in the third person.\n",
    "    Minimize the use of medical jargon and acronyms.\n",
    "    Stop generating when you reach \"--- end of response\".\n",
    "    Do not repeat the instructions in your response.\n",
    "    '''.replace('    ', '')\n",
    "    \n",
    "    context_str = ''\n",
    "    df = discharge_sections_df\n",
    "    cur_df = df[df['hadm_id'] == hadm_id]\n",
    "    cols = ['CC']\n",
    "    context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    context_str = f\"The chief complaint was --- {context_data} \\n\"\n",
    "    cols = ['HPI']\n",
    "    context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    context_str = f\"The history of present illness was --- {context_data} \\n\"\n",
    "    # cols = ['Medication Lists']\n",
    "    # context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    # context_str = context_str + f\"The admission and discharge medication list --- \\n{context_data} \\n\"\n",
    "    cols = ['Transitional Issues']\n",
    "    context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    context_str = context_str + f\"The ongoing issues and followup instructions include ---  \\n{context_data} \\n\"\n",
    "    # cols = ['BHC']\n",
    "    # context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    # context_str = context_str + f\"The Brief Hospital course was:\\n{context_data} \\n\"\n",
    "    cols = ['Discharge Diagnosis']\n",
    "    context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "    context_str = context_str + f\"At discharge, the patient was diagnosed with --- \\n{context_data} \\n\"\n",
    "    context_str = context_str.replace(\":\", '---').replace(\"==\", '')\n",
    "    \n",
    "    # df = diagnosis_df\n",
    "    # cols = ['icd_title']\n",
    "    # cur_df = df[df['hadm_id'] == hadm_id]\n",
    "    # context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n ')\n",
    "    # context_str = context_str + f\"The patient was initially diagnosed with ICD Diagnosis titles --- \\n{context_data}\\n \"\n",
    "    \n",
    "    instruction = f\"{topic} \\n {prompt}\"\n",
    "    context_input = f\"<text> {context_str}</text> \"\n",
    "    \n",
    "    response = generate_with_model(instruction, context_input, model)\n",
    "    return response\n",
    "    \n",
    "# def generate_discharge_instructions(hadm_id):\n",
    "#     topic = '''\n",
    "#     The topic is about clinical notes, medical records, and other text documents from electronic health records (EHR) \n",
    "#     from a patient's hospital admission. \n",
    "#     You will be provided with text and data from a specific patient's medical records and clinical notes\n",
    "#     delimited by xml tags <text> </text>. Use only this data to answer questions about the EHR.  \n",
    "#     Answer clearly and concisely.\n",
    "#     '''.replace('\\n', '')\n",
    "    \n",
    "#     prompt = '''\n",
    "#     Given the following information from a patient's hospital stay,\n",
    "#     Compose a letter to the patient that is courteous and easy to understand. \n",
    "#     There should be limited medical jargon and it should be written in layman's language.  \n",
    "#     The letter will briefly summarize the reason for admission, the treatment course, relevant tests and results.  \n",
    "#     It will also summarize any changes to the patient's current medical management including medication changes. \n",
    "#     Is there any scheduled or recommended follow up?\n",
    "#     If there was a surgery, what are the post-operative or wound care instructions?\n",
    "#     Limit the letter to 3 paragraphs.\n",
    "#     '''.replace('\\n', '')\n",
    "    \n",
    "#     context_str = ''\n",
    "#     df = discharge_sections_df\n",
    "#     cur_df = df[df['hadm_id'] == hadm_id]\n",
    "#     cols = ['HPI']\n",
    "#     context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "#     context_str = f\"The history of present illness was: {context_data} \\n\"\n",
    "#     # cols = ['Medication Lists']\n",
    "#     # context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "#     # context_str = context_str + f\"The admission medication list: \\n{context_data} \\n\"\n",
    "#     cols = ['Transitional Issues']\n",
    "#     context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "#     context_str = context_str + f\"The ongoing issues to be addressed after discharge: \\n{context_data} \\n\"\n",
    "#     # cols = ['BHC']\n",
    "#     # context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n')\n",
    "#     # context_str = context_str + f\"The Brief Hospital course was:\\n{context_data} \\n\"\n",
    "#     df = diagnosis_df\n",
    "#     cols = ['icd_title']\n",
    "#     cur_df = df[df['hadm_id'] == hadm_id]\n",
    "#     context_data = cur_df[cols].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).str.cat(sep='\\n ')\n",
    "#     context_str = context_str + f\"The patient was initially diagnosed with ICD Diagnosis titles: \\n{context_data}\\n \"\n",
    "    \n",
    "    \n",
    "#     instruction = f\"{topic} \\n Instruction: {prompt}\"\n",
    "#     context_input = f\"<text> Text:{context_str}</text> \"\n",
    "    \n",
    "#     response = generate_with_model(instruction, context_input, model)\n",
    "#     discharge_instructions = response\n",
    "#     return discharge_instructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08920cf4-ad8e-494d-b219-0e71261a9378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 10962, 0.009122422915526365% hadm_id: 24962904 **********\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# brief_hospital_course = generate_bhc(hadm_id)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m start_time_di \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 19\u001b[0m discharge_instructions \u001b[38;5;241m=\u001b[39m generate_discharge_instructions(hadm_id)\n\u001b[1;32m     20\u001b[0m new_row \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhadm_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [hadm_id], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdischarge_instructions\u001b[39m\u001b[38;5;124m'\u001b[39m: [discharge_instructions]})\n\u001b[1;32m     21\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([results, new_row], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[18], line 103\u001b[0m, in \u001b[0;36mgenerate_discharge_instructions\u001b[0;34m(hadm_id)\u001b[0m\n\u001b[1;32m    100\u001b[0m instruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m context_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<text> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</text> \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m response \u001b[38;5;241m=\u001b[39m generate_with_model(instruction, context_input, model)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "# Create an empty results table\n",
    "results = pd.DataFrame(columns=['hadm_id', 'discharge_instructions'])\n",
    "\n",
    "# results path:\n",
    "results_path = '../discharge_me/data/scoring/submission_01/'\n",
    "file_name = 'results_discharge_instructions.csv'\n",
    "results_file_path = os.path.join(results_path, file_name)\n",
    "\n",
    "count = 0\n",
    "total_records = len(target_df.hadm_id)\n",
    "for hadm_id in target_df['hadm_id'][:]:  # Replace with your actual hadm_id values\n",
    "    count += 1\n",
    "    percent_complete = count*100/total_records\n",
    "    print(f\"{count} of {total_records}, {percent_complete}% hadm_id: {hadm_id} **********\\n\\n\")\n",
    "    # brief_hospital_course = generate_bhc(hadm_id)\n",
    "    start_time_di = time.time()\n",
    "    discharge_instructions = generate_discharge_instructions(hadm_id)\n",
    "    new_row = pd.DataFrame({'hadm_id': [hadm_id], 'discharge_instructions': [discharge_instructions]})\n",
    "    results = pd.concat([results, new_row], ignore_index=True)\n",
    "    end_time_di = time.time()\n",
    "    duration_di = end_time_di - start_time_di\n",
    "    print(f\"Time taken for generate_discharge_instructions: {duration_di} seconds.\")\n",
    "    # results = results.append({'hadm_id': hadm_id, 'brief_hospital_course': brief_hospital_course, 'discharge_instructions': discharge_instructions}, ignore_index=True)\n",
    "    results.to_csv(results_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09edd6a4-765c-4045-a8d6-4c1e536cbedb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Timing generate_bhc function\u001b[39;00m\n\u001b[1;32m      7\u001b[0m start_time_bhc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 8\u001b[0m brief_hospital_course \u001b[38;5;241m=\u001b[39m generate_bhc(patient_id)\n\u001b[1;32m      9\u001b[0m end_time_bhc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m duration_bhc \u001b[38;5;241m=\u001b[39m end_time_bhc \u001b[38;5;241m-\u001b[39m start_time_bhc\n",
      "Cell \u001b[0;32mIn[17], line 105\u001b[0m, in \u001b[0;36mgenerate_bhc\u001b[0;34m(hadm_id)\u001b[0m\n\u001b[1;32m    102\u001b[0m instruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m context_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<text> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</text> \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 105\u001b[0m response \u001b[38;5;241m=\u001b[39m generate_with_model(instruction, context_input, model)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Patient ID to be processed\n",
    "patient_id = 24962904\n",
    "\n",
    "# Timing generate_bhc function\n",
    "start_time_bhc = time.time()\n",
    "brief_hospital_course = generate_bhc(patient_id)\n",
    "end_time_bhc = time.time()\n",
    "duration_bhc = end_time_bhc - start_time_bhc\n",
    "print(f\"Time taken for generate_bhc: {duration_bhc} seconds.\")\n",
    "\n",
    "# Timing generate_discharge_instructions function\n",
    "start_time_di = time.time()\n",
    "discharge_instructions = generate_discharge_instructions(patient_id)\n",
    "end_time_di = time.time()\n",
    "duration_di = end_time_di - start_time_di\n",
    "print(f\"Time taken for generate_discharge_instructions: {duration_di} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e862362-105a-4351-a2ae-06f1896d499f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bba75b-c8b9-4343-b0b9-148b6d4c9c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
